<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mr. Jing</title>
    <description>Scott&apos;s Homepage</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Sat, 11 Feb 2023 23:24:47 -0500</pubDate>
    <lastBuildDate>Sat, 11 Feb 2023 23:24:47 -0500</lastBuildDate>
    <generator>Jekyll v4.3.1</generator>
    
      <item>
        <title>InAction: Interpretable Action Decision Making for Autonomous Driving</title>
        <description>&lt;h3 id=&quot;abstract&quot;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Autonomous driving attracts lots of interest in interpretable action decision models that mimic human cognition. Existing interpretable autonomous driving models explore static human explanations, which ignore the implicit visual semantics that are not annotated explicitly or even consistent across annotators. In this paper, we propose a novel Interpretable Action decision making (InAction) model to provide an enriched explanation from both explicit human annotation and implicit visual semantics. First, a proposed visual-semantic module captures the region-based action-inducing components from the visual inputs, which automatically learns the implicit visual semantics to provide a human-understandable explanation in action decision making. Second, an explicit reasoning module is developed by incorporating global visual features and action-inducing visual semantics, which aims to jointly align the human-annotated explanation and action decision making in a multi-task fashion. Experimental results on two autonomous driving benchmarks demonstrate the effectiveness of our InAction model for explaining both implicitly and explicitly by comparing it to existing interpretable autonomous driving models.&lt;/p&gt;

&lt;h3 id=&quot;framework&quot;&gt;Framework&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/img/posts/20220703/framework.png&quot; alt=&quot;slides&quot; height=&quot;70%&quot; width=&quot;70%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;explicit-human-annotated-explanation&quot;&gt;Explicit Human-annotated Explanation&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/img/posts/20220703/compare.png&quot; alt=&quot;slides&quot; height=&quot;70%&quot; width=&quot;70%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;visualization-of-learned-prototypes&quot;&gt;Visualization of Learned Prototypes&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/img/posts/20220703/prototype.png&quot; alt=&quot;slides&quot; height=&quot;70%&quot; width=&quot;70%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;implicit-visual-semantic-and-explicit-human-annotated-explainable-action-prediction&quot;&gt;Implicit Visual-Semantic AND Explicit Human-annotated Explainable Action Prediction&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/img/posts/20220703/InAction.png&quot; alt=&quot;slides&quot; height=&quot;70%&quot; width=&quot;70%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;visualization-of-reasoning-process-via-learned-prototypes-activation&quot;&gt;Visualization of Reasoning Process via Learned Prototypes Activation&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/img/posts/20220703/reasoning.png&quot; alt=&quot;slides&quot; height=&quot;70%&quot; width=&quot;70%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;citation&quot;&gt;Citation&lt;/h3&gt;
&lt;p&gt;If you think this work is interesting, feel free to cite&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;@InProceedings{jing2022inaction,
  author = {Jing, Taotao and Xia, Haifeng and Tian, Renran and Ding, Haoran and Luo, Xiao and Domeyer, Joshua and Sherony, Rini and Ding, Zhengming},
  title = {InAction: Interpretable Action Decision Making for Autonomous Driving},
  booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
  month = {October},
  year = {2022}
}

&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;p&gt;[1] Jing, Taotao, Haifeng Xia, Renran Tian, Haoran Ding, Xiao Luo, Joshua Domeyer, Rini Sherony, and Zhengming Ding. “InAction: Interpretable Action Decision Making for Autonomous Driving” &lt;em&gt;In Proceedings of the European Conference on Computer Vision&lt;/em&gt; (&lt;strong&gt;ECCV&lt;/strong&gt;), 2022.&lt;/p&gt;

</description>
        <pubDate>Sun, 03 Jul 2022 13:00:00 -0400</pubDate>
        <link>http://localhost:4000/2022/07/03/InAction/</link>
        <guid isPermaLink="true">http://localhost:4000/2022/07/03/InAction/</guid>
        
        <category>Interpretable AI</category>
        
        <category>Autonomous Driving</category>
        
        <category>Multimodal Learning</category>
        
        
      </item>
    
      <item>
        <title>Maximum Structural Generation Discrepancy for Unsupervised Domain Adaptation</title>
        <description>&lt;h3 id=&quot;abstract&quot;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Unsupervised domain adaptation (UDA) has recently become an appealing research topic in visual recognition, since it exploits all accessible well-labeled source data to train a model with high generalization on target domain without any annotations. However, due to the significant domain discrepancy, the bottleneck for UDA is to learn effective domain-invariant feature representations. To fight off such an obstacle, we propose a novel cross-domain learning framework named Maximum Structural Generation Discrepancy (MSGD) to accurately estimate and mitigate domain shift via introducing an intermediate domain. First, the cross-domain topological structure is explored to propagate target samples to generate a novel intermediate domain paired with the specific source instances. The intermediate domain plays as the bridge to gradually reduce distribution divergence across source and target domains. Concretely, the similar category semantic across source and intermediate features tends to naturally conduct the class-level alignment on eliminating their domain shift. In terms of no target annotation, the domain-level alignment manner is suitable to narrow down the distance between intermediate and target domains. Moreover, to produce high-quality generative instances, we develop the class-driven collaborative translation (CDCT) module to generate class-consistent cross-domain samples in each mini-batch with the assistance of pseudo-labels. Extensive experimental analyses on five domain adaptation benchmarks demonstrate the effectiveness of our MSGD on solving UDA problem.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/posts/20220603/figure1.png&quot; alt=&quot;slides&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;
&lt;img src=&quot;/img/posts/20220603/MSGD.png&quot; alt=&quot;slides&quot; height=&quot;100%&quot; width=&quot;100%&quot; /&gt;
&lt;img src=&quot;/img/posts/20220603/figure3.png&quot; alt=&quot;slides&quot; height=&quot;100%&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;citation&quot;&gt;Citation&lt;/h3&gt;
&lt;p&gt;If you think this work is interesting, feel free to cite&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;@ARTICLE{xia2022maximum,
  author={Xia, Haifeng and Jing, Taotao and Ding, Zhengming},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Maximum Structural Generation Discrepancy for Unsupervised Domain Adaptation}, 
  year={2022},
  volume={},
  number={},
  pages={1-1},
  doi={10.1109/TPAMI.2022.3174526}}

&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;p&gt;[1] H. Xia, T. Jing and Z. Ding, “Maximum Structural Generation Discrepancy for Unsupervised Domain Adaptation,” in IEEE Transactions on Pattern Analysis and Machine Intelligence, doi: 10.1109/TPAMI.2022.3174526.&lt;/p&gt;

</description>
        <pubDate>Fri, 03 Jun 2022 13:00:00 -0400</pubDate>
        <link>http://localhost:4000/2022/06/03/MSGD/</link>
        <guid isPermaLink="true">http://localhost:4000/2022/06/03/MSGD/</guid>
        
        <category>Domain Adaptation</category>
        
        <category>Transfer Learning</category>
        
        <category>Unsupervised Domain Adaptation</category>
        
        <category>Structural Generation</category>
        
        <category>Collaborative Translation</category>
        
        
      </item>
    
      <item>
        <title>Augmented Multi-Modality Fusion for Generalized Zero-Shot Sketch-based Visual Retrieval</title>
        <description>&lt;h3 id=&quot;abstract&quot;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Zero-shot sketch-based image retrieval (ZS-SBIR) has attracted great attention recently, due to the potential application of sketch-based retrieval under zero-shot scenarios, where the categories of query sketches and gallery photos are not observed in the training stage. However, it is still under insufficient exploration for the general and practical scenario when the query sketches and gallery photos contain both seen and unseen categories. Such a problem is defined as generalized zero- shot sketch-based image retrieval (GZS-SBIR), which is the focus of this work. To this end, we propose a novel Augmented Multi- modality Fusion (AMF) framework to generalize seen concepts to unobserved ones efficiently. Specifically, a novel knowledge discovery module named cross-domain augmentation is designed in both visual and semantic space to mimic novel knowledge unseen from the training stage, which is the key to handling the GZS-SBIR challenge. Moreover, a triplet domain alignment module is proposed to couple the cross-domain distribution between photo and sketch in visual space. To enhance the robustness of our model, we explore embedding propagation to refine both visual and semantic features by removing un- desired noise. Eventually, visual-semantic fusion representations are concatenated for further domain discrimination and task- specific recognition, which tend to trigger the cross-domain align- ment in both visual and semantic feature space. Experimental evaluations are conducted on popular ZS-SBIR benchmarks as well as a new evaluation protocol designed for GZS-SBIR from DomainNet dataset with more diverse sub-domains, and the promising results demonstrate the superiority of the proposed solution over other baselines. The source code is available at https://github.com/scottjingtt/AMF GZS SBIR.git.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/posts/20220502/figure1.png&quot; alt=&quot;slides&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;
&lt;img src=&quot;/img/posts/20220502/figure2.png&quot; alt=&quot;slides&quot; height=&quot;100%&quot; width=&quot;100%&quot; /&gt;
&lt;img src=&quot;/img/posts/20220502/figure3.png&quot; alt=&quot;slides&quot; height=&quot;100%&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;citation&quot;&gt;Citation&lt;/h3&gt;
&lt;p&gt;If you think this work is interesting, feel free to cite&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;@article{jing2021augmented,
  title={Augmented Multi-Modality Fusion for Generalized Zero-Shot Sketch-based Visual Retrieval},
  author={Jing, Taotao and Xia, Haifeng and Hamm, Jihun and Ding, Zhengming},
  journal={IEEE Transactions on Image Processing},
  volume={},
  pages={},
  year={2022},
  publisher={IEEE}
}
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;p&gt;[1] Jing, Taotao, Haifeng Xia, Jihun Hamm, Zhengming Ding. “Augmented Multi-Modality Fusion for Generalized Zero-Shot Sketch-based Visual Retrieval.” IEEE Transactions on Image Processing (2022).&lt;/p&gt;

</description>
        <pubDate>Mon, 02 May 2022 13:00:00 -0400</pubDate>
        <link>http://localhost:4000/2022/05/02/AMF/</link>
        <guid isPermaLink="true">http://localhost:4000/2022/05/02/AMF/</guid>
        
        <category>Generalized Zero-shot Learning</category>
        
        <category>Zero-shot Learning</category>
        
        <category>Transfer Learning</category>
        
        <category>Image Retrieval</category>
        
        <category>Multimodal Learning</category>
        
        <category>Dataset</category>
        
        
      </item>
    
      <item>
        <title>Milestone! - Ph.D. Oral Qualifying Exam</title>
        <description>&lt;h2 id=&quot;sincerely-appreciate-the-time-of-all-my-phd-committee-members&quot;&gt;Sincerely appreciate the time of all my Ph.D. committee members!&lt;/h2&gt;

&lt;p&gt;We had a great and inspiring discussion about transfer learning and domain adaptation. &lt;strong&gt;Finally&lt;/strong&gt;, I passed the oral qualifying exam and became a &lt;strong&gt;Ph.D. candidate&lt;/strong&gt; officially!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/posts/20220427/slides1.jpg&quot; alt=&quot;slides&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;
&lt;img src=&quot;/img/posts/20220427/slides2.jpg&quot; alt=&quot;slides&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;
&lt;img src=&quot;/img/posts/20220427/talk1.jpg&quot; alt=&quot;slides&quot; height=&quot;50%&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 27 Apr 2022 13:00:00 -0400</pubDate>
        <link>http://localhost:4000/2022/04/27/QualifyingExam/</link>
        <guid isPermaLink="true">http://localhost:4000/2022/04/27/QualifyingExam/</guid>
        
        <category>Ph.D.</category>
        
        <category>Talk</category>
        
        <category>Transfer Learning</category>
        
        
      </item>
    
      <item>
        <title>Towards Fair Knowledge Transfer for Imbalanced Domain Adaptation</title>
        <description>&lt;h3 id=&quot;tfkt&quot;&gt;TFKT&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/img/research/TFKT.png&quot; alt=&quot;TFKT&quot; height=&quot;100%&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;citation&quot;&gt;Citation&lt;/h3&gt;
&lt;p&gt;If you think this work is interesting, feel free to cite&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;@article{jing2021towards,
  title={Towards Fair Knowledge Transfer for Imbalanced Domain Adaptation},
  author={Jing, Taotao and Xu, Bingrong and Ding, Zhengming},
  journal={IEEE Transactions on Image Processing},
  volume={30},
  pages={8200--8211},
  year={2021},
  publisher={IEEE}
}
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;p&gt;[1] Jing, Taotao, Bingrong Xu, and Zhengming Ding. “Towards Fair Knowledge Transfer for Imbalanced Domain Adaptation.” IEEE Transactions on Image Processing 30 (2021): 8200-8211.&lt;/p&gt;
</description>
        <pubDate>Fri, 11 Mar 2022 11:00:00 -0500</pubDate>
        <link>http://localhost:4000/2022/03/11/TFKT/</link>
        <guid isPermaLink="true">http://localhost:4000/2022/03/11/TFKT/</guid>
        
        <category>Fairness Learning</category>
        
        <category>Domain Adaptation</category>
        
        <category>Transfer Learning</category>
        
        
      </item>
    
      <item>
        <title>Towards Novel Target Discovery Through Open-Set Domain Adaptation</title>
        <description>&lt;h3 id=&quot;tfkt&quot;&gt;TFKT&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/img/research/SROSDA.png&quot; alt=&quot;SROSDA&quot; height=&quot;100%&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;citation&quot;&gt;Citation&lt;/h3&gt;
&lt;p&gt;If you think this work is interesting, feel free to cite&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;@inproceedings{jing2021towards,
  title={Towards novel target discovery through open-set domain adaptation},
  author={Jing, Taotao and Liu, Hongfu and Ding, Zhengming},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={9322--9331},
  year={2021}
}

&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;p&gt;[1] Jing, Taotao, Hongfu Liu, and Zhengming Ding. “Towards novel target discovery through open-set domain adaptation.” In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9322-9331. 2021.&lt;/p&gt;
</description>
        <pubDate>Fri, 11 Mar 2022 11:00:00 -0500</pubDate>
        <link>http://localhost:4000/2022/03/11/SROSDA/</link>
        <guid isPermaLink="true">http://localhost:4000/2022/03/11/SROSDA/</guid>
        
        <category>Domain Adaptation</category>
        
        <category>Open-set Domain Adaptation</category>
        
        <category>Transfer Learning</category>
        
        <category>Multimodal Learning</category>
        
        
      </item>
    
      <item>
        <title>PSI: A Pedestrian Behavior Dataset for Socially Intelligent Autonomous Car</title>
        <description>&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;Prediction of pedestrian behavior is critical for fully autonomous vehicles to drive in busy city streets safely and efficiently. The future autonomous cars need to fit into mixed conditions with not only technical but also social capabilities. As more algorithms and datasets have been developed to predict pedestrian behaviors, these efforts lack the benchmark labels and the capability to estimate the temporal-dynamic intent changes of the pedestrians, provide explanations of the interaction scenes, and support algorithms with social intelligence. This paper pro- poses and shares another benchmark dataset called the IUPUI- CSRC Pedestrian Situated Intent (PSI) data with two innovative labels besides comprehensive computer vision labels. The first novel label is the dynamic intent changes for the pedestrians to cross in front of the ego-vehicle, achieved from 24 drivers with diverse backgrounds. The second one is the text-based explanations of the driver reasoning process when estimating pedestrian intents and predicting their behaviors during the interaction period. These innovative labels can enable several computer vision tasks, including pedestrian intent/behavior pre- diction, vehicle-pedestrian interaction segmentation, and video- to-language mapping for explainable algorithms. The released dataset can fundamentally improve the development of pedestrian behavior prediction models and develop socially intelligent au- tonomous cars to interact with pedestrians efficiently. The dataset has been evaluated with different tasks and is released to the public to access.&lt;/p&gt;

&lt;h3 id=&quot;psi&quot;&gt;PSI&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/img/research/PSI.png&quot; alt=&quot;PSI&quot; height=&quot;100%&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;dataset-website&quot;&gt;Dataset &lt;a href=&quot;http://situated-intent.net/pedestrian_dataset/&quot;&gt;[Website]&lt;/a&gt;&lt;/h3&gt;

&lt;h3 id=&quot;code-github&quot;&gt;Code &lt;a href=&quot;https://github.com/PSI-Intention2022/PSI-Intention&quot;&gt;[GitHub]&lt;/a&gt;&lt;/h3&gt;

&lt;h3 id=&quot;more-resource---situated-intent&quot;&gt;More Resource - &lt;a href=&quot;http://situated-intent.net&quot;&gt;[Situated Intent]&lt;/a&gt;&lt;/h3&gt;

&lt;h3 id=&quot;citation&quot;&gt;Citation&lt;/h3&gt;
&lt;p&gt;If you think this work is interesting, feel free to cite&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;@article{chen2021psi,
  title={PSI: A Pedestrian Behavior Dataset for Socially Intelligent Autonomous Car},
  author={Chen, Tina and Tian, Renran and Chen, Yaobin and Domeyer, Joshua and Toyoda, Heishiro and Sherony, Rini and Jing, Taotao and Ding, Zhengming},
  journal={arXiv preprint arXiv:2112.02604},
  year={2021}
}


&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;p&gt;[1] Chen, Tina, Renran Tian, Yaobin Chen, Joshua Domeyer, Heishiro Toyoda, Rini Sherony, Taotao Jing, and Zhengming Ding. “PSI: A Pedestrian Behavior Dataset for Socially Intelligent Autonomous Car.” arXiv preprint arXiv:2112.02604 (2021).&lt;/p&gt;
</description>
        <pubDate>Fri, 11 Mar 2022 11:00:00 -0500</pubDate>
        <link>http://localhost:4000/2022/03/11/PSI/</link>
        <guid isPermaLink="true">http://localhost:4000/2022/03/11/PSI/</guid>
        
        <category>Autonomous Driving</category>
        
        <category>Pedestrian Intention Estimation</category>
        
        <category>Trajectory Prediction</category>
        
        <category>Interpretable AI</category>
        
        <category>Dataset</category>
        
        
      </item>
    
      <item>
        <title>EV-Action: Electromyography-Vision Multi-Modal Action Dataset</title>
        <description>&lt;h3 id=&quot;tfkt&quot;&gt;TFKT&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/img/research/EV.png&quot; alt=&quot;EV&quot; height=&quot;100%&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;citation&quot;&gt;Citation&lt;/h3&gt;
&lt;p&gt;If you think this work is interesting, feel free to cite&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;@inproceedings{wang2020ev,
  title={Ev-action: Electromyography-vision multi-modal action dataset},
  author={Wang, Lichen and Sun, Bin and Robinson, Joseph and Jing, Taotao and Fu, Yun},
  booktitle={2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)},
  pages={160--167},
  year={2020},
  organization={IEEE}
}
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;p&gt;[1] Wang, Lichen, Bin Sun, Joseph Robinson, Taotao Jing, and Yun Fu. “Ev-action: Electromyography-vision multi-modal action dataset.” In 2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020), pp. 160-167. IEEE, 2020.&lt;/p&gt;
</description>
        <pubDate>Fri, 11 Mar 2022 11:00:00 -0500</pubDate>
        <link>http://localhost:4000/2022/03/11/EVAction/</link>
        <guid isPermaLink="true">http://localhost:4000/2022/03/11/EVAction/</guid>
        
        <category>Multi-Modal Learning</category>
        
        <category>Action Recognition</category>
        
        <category>Dataset</category>
        
        
      </item>
    
      <item>
        <title>Adversarial Dual Distinct Classifiers for Unsupervised Domain Adaptation</title>
        <description>&lt;h3 id=&quot;tfkt&quot;&gt;TFKT&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/img/research/ADCN.png&quot; alt=&quot;ADCN&quot; height=&quot;100%&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;citation&quot;&gt;Citation&lt;/h3&gt;
&lt;p&gt;If you think this work is interesting, feel free to cite&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;@inproceedings{jing2021adversarial,
  title={Adversarial dual distinct classifiers for unsupervised domain adaptation},
  author={Jing, Taotao and Ding, Zhengming},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={605--614},
  year={2021}
}
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;p&gt;[1] Jing, Taotao, and Zhengming Ding. “Adversarial dual distinct classifiers for unsupervised domain adaptation.” In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 605-614. 2021.&lt;/p&gt;
</description>
        <pubDate>Fri, 11 Mar 2022 11:00:00 -0500</pubDate>
        <link>http://localhost:4000/2022/03/11/ADCN/</link>
        <guid isPermaLink="true">http://localhost:4000/2022/03/11/ADCN/</guid>
        
        <category>Domain Adaptation</category>
        
        <category>Transfer Learning</category>
        
        
      </item>
    
      <item>
        <title>Adaptively-Accumulated Knowledge Transfer for Partial Domain Adaptation</title>
        <description>&lt;h3 id=&quot;tfkt&quot;&gt;TFKT&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/img/research/AAKT.png&quot; alt=&quot;AAKT&quot; height=&quot;100%&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;citation&quot;&gt;Citation&lt;/h3&gt;
&lt;p&gt;If you think this work is interesting, feel free to cite&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;@inproceedings{jing2020adaptively,
  title={Adaptively-accumulated knowledge transfer for partial domain adaptation},
  author={Jing, Taotao and Xia, Haifeng and Ding, Zhengming},
  booktitle={Proceedings of the 28th ACM International Conference on Multimedia},
  pages={1606--1614},
  year={2020}
}
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;p&gt;[1] Jing, Taotao, Haifeng Xia, and Zhengming Ding. “Adaptively-accumulated knowledge transfer for partial domain adaptation.” In Proceedings of the 28th ACM International Conference on Multimedia, pp. 1606-1614. 2020.&lt;/p&gt;
</description>
        <pubDate>Fri, 11 Mar 2022 11:00:00 -0500</pubDate>
        <link>http://localhost:4000/2022/03/11/AAKT/</link>
        <guid isPermaLink="true">http://localhost:4000/2022/03/11/AAKT/</guid>
        
        <category>Domain Adaptation</category>
        
        <category>Partial Domain Adaptation</category>
        
        <category>Transfer Learning</category>
        
        
      </item>
    
  </channel>
</rss>
