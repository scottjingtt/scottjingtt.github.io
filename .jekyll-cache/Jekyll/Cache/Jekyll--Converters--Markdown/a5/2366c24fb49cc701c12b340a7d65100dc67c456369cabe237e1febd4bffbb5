I"<h3 id="abstract">Abstract</h3>
<p>Zero-shot sketch-based image retrieval (ZS-SBIR) has attracted great attention recently, due to the potential application of sketch-based retrieval under zero-shot scenarios, where the categories of query sketches and gallery photos are not observed in the training stage. However, it is still under insufficient exploration for the general and practical scenario when the query sketches and gallery photos contain both seen and unseen categories. Such a problem is defined as generalized zero- shot sketch-based image retrieval (GZS-SBIR), which is the focus of this work. To this end, we propose a novel Augmented Multi- modality Fusion (AMF) framework to generalize seen concepts to unobserved ones efficiently. Specifically, a novel knowledge discovery module named cross-domain augmentation is designed in both visual and semantic space to mimic novel knowledge unseen from the training stage, which is the key to handling the GZS-SBIR challenge. Moreover, a triplet domain alignment module is proposed to couple the cross-domain distribution between photo and sketch in visual space. To enhance the robustness of our model, we explore embedding propagation to refine both visual and semantic features by removing un- desired noise. Eventually, visual-semantic fusion representations are concatenated for further domain discrimination and task- specific recognition, which tend to trigger the cross-domain align- ment in both visual and semantic feature space. Experimental evaluations are conducted on popular ZS-SBIR benchmarks as well as a new evaluation protocol designed for GZS-SBIR from DomainNet dataset with more diverse sub-domains, and the promising results demonstrate the superiority of the proposed solution over other baselines. The source code is available at https://github.com/scottjingtt/AMF GZS SBIR.git.</p>

<p><img src="/img/posts/20220502/figure1.png" alt="slides" height="50%" width="50%" />
<img src="/img/posts/20220502/figure2.png" alt="slides" height="100%" width="100%" />
<img src="/img/posts/20220502/figure3.png" alt="slides" height="100%" width="100%" /></p>

<h3 id="citation">Citation</h3>
<p>If you think this work is interesting, feel free to cite</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="rouge-code"><pre>@article{jing2021augmented,
  title={Augmented Multi-Modality Fusion for Generalized Zero-Shot Sketch-based Visual Retrieval},
  author={Jing, Taotao and Xia, Haifeng and Hamm, Jihun and Ding, Zhengming},
  journal={IEEE Transactions on Image Processing},
  volume={},
  pages={},
  year={2022},
  publisher={IEEE}
}
</pre></td></tr></tbody></table></code></pre></div></div>

<h3 id="reference">Reference</h3>

<p>[1] Jing, Taotao, Bingrong Xu, and Zhengming Ding. “Towards Fair Knowledge Transfer for Imbalanced Domain Adaptation.” IEEE Transactions on Image Processing 30 (2021): 8200-8211.</p>

:ET